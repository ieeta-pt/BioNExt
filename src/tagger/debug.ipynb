{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biocreative/BioCreativeVIII_Track1/venv-linking/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "from data import load_train_test_split, load_old_train_test_split, SelectModelInputs, BIOTagger, RandomlyUKNTokens, EvaluationDataCollator, RandomlyReplaceTokens\n",
    "from transformers import TrainingArguments, AutoTokenizer, DataCollatorForTokenClassification\n",
    "\n",
    "from model.configuration_bionexttager import BioNExtTaggerConfig\n",
    "from model.modeling_bionexttagger import BioNExtTaggerModel\n",
    "from trainer import NERTrainer\n",
    "from metrics import NERMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"michiyasunaga/BioLinkBERT-large\"\n",
    "\n",
    "CONTEXT_SIZE = 64\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "biotagger = BIOTagger()\n",
    "transforms = [biotagger, SelectModelInputs()]\n",
    "\n",
    "train_augmentation = None\n",
    "\n",
    "train_ds, test_ds = load_old_train_test_split(\"../../dataset/\",\n",
    "                                          tokenizer=tokenizer,\n",
    "                                          context_size=CONTEXT_SIZE,\n",
    "                                          train_transformations=transforms,\n",
    "                                          train_augmentations=train_augmentation,\n",
    "                                          test_transformations=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type crf-tagger. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BioNExtTaggerModel were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'crf.end_transitions', 'crf.start_transitions', 'crf.transitions', 'dense.bias', 'dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0:\"O\", \n",
    "        1:\"B-GeneOrGeneProduct\", 2:\"I-GeneOrGeneProduct\",\n",
    "        3:\"B-DiseaseOrPhenotypicFeature\", 4:\"I-DiseaseOrPhenotypicFeature\",\n",
    "        5:\"B-ChemicalEntity\", 6:\"I-ChemicalEntity\",\n",
    "        7:\"B-SequenceVariant\", 8:\"I-SequenceVariant\",\n",
    "        9:\"B-OrganismTaxon\", 10:\"I-OrganismTaxon\",\n",
    "        11:\"B-CellLine\", 12:\"I-CellLine\"}\n",
    "\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "\n",
    "\n",
    "config = BioNExtTaggerConfig.from_pretrained(model_checkpoint,\n",
    "                                                id2label = id2label,\n",
    "                                                label2id = label2id,\n",
    "                                                augmentation = None,\n",
    "                                                context_size = CONTEXT_SIZE,\n",
    "                                                percentage_tags = 0.2,\n",
    "                                                p_augmentation = 0.5,\n",
    "                                                freeze = False,\n",
    "                                                crf_reduction = \"mean\")\n",
    "\n",
    "model = BioNExtTaggerModel.from_pretrained(model_checkpoint, config=config)\n",
    "model.training_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dl = torch.utils.data.DataLoader(train_ds, \n",
    "                                 batch_size=8,\n",
    "                                 collate_fn=DataCollatorForTokenClassification(tokenizer=tokenizer, \n",
    "                                                        padding=\"longest\",\n",
    "                                                        label_pad_token_id=tokenizer.pad_token_id),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1112.5940, grad_fn=<MeanBackward0>),\n",
       " tensor([[[ 0.1314,  0.1425, -0.1065,  ...,  0.2232, -0.0677,  0.0995],\n",
       "          [-0.2288,  0.1166,  0.3832,  ...,  0.2447,  0.0535, -0.0102],\n",
       "          [-0.2490,  0.1743,  0.3375,  ...,  0.1630, -0.0473,  0.0080],\n",
       "          ...,\n",
       "          [ 0.0009,  0.0647,  0.0020,  ..., -0.0402, -0.1703,  0.3503],\n",
       "          [ 0.0521,  0.1442, -0.0608,  ...,  0.2008,  0.0634,  0.1576],\n",
       "          [ 0.1314,  0.1425, -0.1065,  ...,  0.2232, -0.0677,  0.0995]],\n",
       " \n",
       "         [[ 0.1209,  0.1868, -0.0999,  ...,  0.2724, -0.0539,  0.0797],\n",
       "          [-0.0904,  0.2252,  0.2537,  ...,  0.3074, -0.1605,  0.0993],\n",
       "          [-0.1693,  0.1520,  0.1501,  ...,  0.2120, -0.0771,  0.1270],\n",
       "          ...,\n",
       "          [-0.0803,  0.1711,  0.1855,  ...,  0.2423, -0.1174,  0.2133],\n",
       "          [-0.0357,  0.1522,  0.1749,  ...,  0.2545, -0.0666,  0.2004],\n",
       "          [-0.1062,  0.2567,  0.2339,  ...,  0.3484, -0.0575,  0.1502]],\n",
       " \n",
       "         [[ 0.2488,  0.0982, -0.1258,  ...,  0.1433,  0.0913,  0.1463],\n",
       "          [-0.1943,  0.2021,  0.3028,  ...,  0.1447, -0.0201,  0.0681],\n",
       "          [-0.1566,  0.1555,  0.2661,  ...,  0.1559, -0.0672,  0.0761],\n",
       "          ...,\n",
       "          [ 0.0570,  0.1007,  0.2132,  ...,  0.2101, -0.0472,  0.2161],\n",
       "          [-0.0659,  0.0967,  0.2311,  ...,  0.2358, -0.1128,  0.1928],\n",
       "          [-0.0555,  0.0898,  0.1961,  ...,  0.2482, -0.0856,  0.1555]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.1241,  0.1694, -0.0887,  ...,  0.1988,  0.0205,  0.1843],\n",
       "          [-0.3167,  0.1644,  0.3209,  ...,  0.1196, -0.0462,  0.0682],\n",
       "          [-0.3124,  0.1871,  0.3275,  ...,  0.0974, -0.0824,  0.0528],\n",
       "          ...,\n",
       "          [-0.0874,  0.1667,  0.2621,  ...,  0.2870, -0.1971,  0.1659],\n",
       "          [-0.0814,  0.1789,  0.1994,  ...,  0.1837, -0.1400,  0.1687],\n",
       "          [-0.1692,  0.0954,  0.1870,  ...,  0.2268, -0.0109,  0.0799]],\n",
       " \n",
       "         [[ 0.0349,  0.1838,  0.0259,  ...,  0.1940, -0.0869,  0.2376],\n",
       "          [-0.2079,  0.2078,  0.3376,  ...,  0.2515, -0.0792,  0.1152],\n",
       "          [-0.2024,  0.2253,  0.3410,  ...,  0.2310, -0.1129,  0.1050],\n",
       "          ...,\n",
       "          [-0.2202,  0.1715,  0.3400,  ...,  0.2689, -0.1444,  0.1214],\n",
       "          [-0.1453,  0.1086,  0.1794,  ...,  0.1670, -0.2249,  0.1320],\n",
       "          [-0.0706,  0.1064,  0.1546,  ...,  0.2616, -0.1610,  0.2725]],\n",
       " \n",
       "         [[ 0.0997,  0.0228, -0.1371,  ...,  0.2934,  0.0333,  0.1368],\n",
       "          [-0.2650,  0.0336,  0.2936,  ...,  0.1789, -0.0943,  0.0802],\n",
       "          [-0.2673, -0.0046,  0.2825,  ...,  0.2065, -0.1283,  0.1577],\n",
       "          ...,\n",
       "          [-0.1917,  0.1928,  0.1833,  ...,  0.1291, -0.0443,  0.1988],\n",
       "          [-0.1592,  0.1676,  0.1862,  ...,  0.2610, -0.1002,  0.1602],\n",
       "          [-0.2458,  0.1691,  0.2493,  ...,  0.3503, -0.0598,  0.1068]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2= BioNExtTaggerModel.from_pretrained(\"IEETA/BioNExt-Tagger\", cache_dir=\"../../trained_models/tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2666, grad_fn=<MeanBackward0>),\n",
       " tensor([[[ 4.7006e+00, -6.5339e+00,  5.2031e-01,  ..., -2.5788e-01,\n",
       "           -6.9013e+00,  2.4141e+00],\n",
       "          [ 4.2888e+00, -6.6116e+00,  9.3731e-01,  ..., -2.6071e-03,\n",
       "           -7.0781e+00,  2.5486e+00],\n",
       "          [ 4.2261e+00, -6.6249e+00,  1.0259e+00,  ...,  5.0407e-02,\n",
       "           -7.1159e+00,  2.5762e+00],\n",
       "          ...,\n",
       "          [ 5.8322e+00, -6.0646e+00, -5.6425e-01,  ..., -9.5071e-01,\n",
       "           -6.2811e+00,  1.8675e+00],\n",
       "          [ 4.4670e+00, -6.6336e+00,  7.8425e-01,  ..., -7.5192e-02,\n",
       "           -7.0676e+00,  2.5124e+00],\n",
       "          [ 4.2491e+00, -5.7781e+00,  1.6972e-01,  ..., -2.6779e-01,\n",
       "           -6.3762e+00,  2.1115e+00]],\n",
       " \n",
       "         [[ 4.3542e+00, -6.6502e+00,  9.2685e-01,  ..., -2.2874e-02,\n",
       "           -7.0973e+00,  2.5447e+00],\n",
       "          [ 5.7428e+00, -5.8342e+00, -3.1436e-01,  ..., -9.6783e-01,\n",
       "           -6.0599e+00,  1.9640e+00],\n",
       "          [ 3.4926e+00, -6.0575e+00,  1.0011e+00,  ...,  1.5342e-01,\n",
       "           -6.3571e+00,  2.4486e+00],\n",
       "          ...,\n",
       "          [ 4.2501e+00, -6.6542e+00,  1.0574e+00,  ...,  5.2562e-02,\n",
       "           -7.1399e+00,  2.5750e+00],\n",
       "          [ 4.2550e+00, -6.6547e+00,  1.0515e+00,  ...,  4.9056e-02,\n",
       "           -7.1391e+00,  2.5738e+00],\n",
       "          [ 4.2357e+00, -6.6540e+00,  1.0679e+00,  ...,  5.9275e-02,\n",
       "           -7.1430e+00,  2.5762e+00]],\n",
       " \n",
       "         [[ 4.7251e+00, -6.5022e+00,  4.1187e-01,  ..., -3.8020e-01,\n",
       "           -6.8391e+00,  2.3364e+00],\n",
       "          [ 4.9214e+00, -6.4420e+00,  2.2958e-01,  ..., -5.0920e-01,\n",
       "           -6.7325e+00,  2.2512e+00],\n",
       "          [ 4.7562e+00, -6.4932e+00,  3.8079e-01,  ..., -4.0450e-01,\n",
       "           -6.8173e+00,  2.3184e+00],\n",
       "          ...,\n",
       "          [ 4.5436e+00, -6.5546e+00,  6.3005e-01,  ..., -2.4199e-01,\n",
       "           -6.9745e+00,  2.4223e+00],\n",
       "          [ 4.5061e+00, -6.5603e+00,  6.4837e-01,  ..., -2.2593e-01,\n",
       "           -6.9770e+00,  2.4239e+00],\n",
       "          [ 4.5025e+00, -6.5626e+00,  6.5435e-01,  ..., -2.2284e-01,\n",
       "           -6.9804e+00,  2.4264e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 4.6444e+00, -6.4706e+00,  4.9122e-01,  ..., -3.4444e-01,\n",
       "           -6.8164e+00,  2.3682e+00],\n",
       "          [ 4.8218e+00, -6.4164e+00,  3.0449e-01,  ..., -4.6082e-01,\n",
       "           -6.7177e+00,  2.2732e+00],\n",
       "          [ 4.7044e+00, -6.4536e+00,  4.3083e-01,  ..., -3.8413e-01,\n",
       "           -6.7820e+00,  2.3338e+00],\n",
       "          ...,\n",
       "          [ 4.6220e+00, -6.4871e+00,  5.5048e-01,  ..., -3.0638e-01,\n",
       "           -6.8872e+00,  2.3684e+00],\n",
       "          [ 4.5899e+00, -6.4831e+00,  5.8194e-01,  ..., -2.9250e-01,\n",
       "           -6.8920e+00,  2.3722e+00],\n",
       "          [ 4.5609e+00, -6.4867e+00,  6.1644e-01,  ..., -2.7547e-01,\n",
       "           -6.9089e+00,  2.3748e+00]],\n",
       " \n",
       "         [[ 4.7461e+00, -6.5370e+00,  4.3244e-01,  ..., -3.4709e-01,\n",
       "           -6.8948e+00,  2.3510e+00],\n",
       "          [ 4.6878e+00, -6.5557e+00,  4.8835e-01,  ..., -3.1056e-01,\n",
       "           -6.9178e+00,  2.3670e+00],\n",
       "          [ 4.6474e+00, -6.5643e+00,  5.1445e-01,  ..., -2.8230e-01,\n",
       "           -6.9328e+00,  2.3859e+00],\n",
       "          ...,\n",
       "          [ 4.6232e+00, -6.5330e+00,  5.1518e-01,  ..., -2.6750e-01,\n",
       "           -6.9271e+00,  2.3751e+00],\n",
       "          [ 4.6405e+00, -6.5330e+00,  4.9459e-01,  ..., -2.7568e-01,\n",
       "           -6.9178e+00,  2.3767e+00],\n",
       "          [ 4.6803e+00, -6.5280e+00,  4.5260e-01,  ..., -2.9853e-01,\n",
       "           -6.8997e+00,  2.3678e+00]],\n",
       " \n",
       "         [[ 4.6709e+00, -6.4823e+00,  5.6468e-01,  ..., -3.2466e-01,\n",
       "           -6.8994e+00,  2.3635e+00],\n",
       "          [ 4.6061e+00, -6.4948e+00,  6.2119e-01,  ..., -2.8952e-01,\n",
       "           -6.9182e+00,  2.3787e+00],\n",
       "          [ 4.6075e+00, -6.4911e+00,  6.3728e-01,  ..., -2.8368e-01,\n",
       "           -6.9296e+00,  2.3861e+00],\n",
       "          ...,\n",
       "          [ 4.5971e+00, -6.4434e+00,  7.2074e-01,  ..., -2.5942e-01,\n",
       "           -6.9832e+00,  2.3758e+00],\n",
       "          [ 4.6437e+00, -6.4497e+00,  6.5788e-01,  ..., -2.8248e-01,\n",
       "           -6.9636e+00,  2.3654e+00],\n",
       "          [ 4.5695e+00, -6.4631e+00,  7.0938e-01,  ..., -2.4668e-01,\n",
       "           -6.9839e+00,  2.3864e+00]]], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2(**sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model1, model2):\n",
    "    for (name1, module1), (name2, module2) in zip(model1.named_modules(), model2.named_modules()):\n",
    "        print(f\"Comparing Layer: {name1} -> {name2}\")\n",
    "        print(f\"Model 1: {module1}\")\n",
    "        print(f\"Model 2: {module2}\")\n",
    "        if hasattr(module1, 'weight') and hasattr(module2, 'weight'):\n",
    "            print(f\"Weights: {module1.weight.shape} vs {module2.weight.shape}\")\n",
    "        if hasattr(module1, 'bias') and hasattr(module2, 'bias'):\n",
    "            print(f\"Biases: {module1.bias.shape if module1.bias is not None else 'None'} vs {module2.bias.shape if module2.bias is not None else 'None'}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Layer:  -> \n",
      "Model 1: BioNExtTaggerModel(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28895, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dense_activation): GELU(approximate='none')\n",
      "  (classifier): Linear(in_features=1024, out_features=13, bias=True)\n",
      "  (crf): CRF(num_tags=13)\n",
      ")\n",
      "Model 2: BioNExtTaggerModel(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28895, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dense_activation): GELU(approximate='none')\n",
      "  (classifier): Linear(in_features=1024, out_features=13, bias=True)\n",
      "  (crf): CRF(num_tags=13)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert -> bert\n",
      "Model 1: BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28895, 1024, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 1024)\n",
      "    (token_type_embeddings): Embedding(2, 1024)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-23): 24 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model 2: BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28895, 1024, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 1024)\n",
      "    (token_type_embeddings): Embedding(2, 1024)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-23): 24 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.embeddings -> bert.embeddings\n",
      "Model 1: BertEmbeddings(\n",
      "  (word_embeddings): Embedding(28895, 1024, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 1024)\n",
      "  (token_type_embeddings): Embedding(2, 1024)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertEmbeddings(\n",
      "  (word_embeddings): Embedding(28895, 1024, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 1024)\n",
      "  (token_type_embeddings): Embedding(2, 1024)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.embeddings.word_embeddings -> bert.embeddings.word_embeddings\n",
      "Model 1: Embedding(28895, 1024, padding_idx=0)\n",
      "Model 2: Embedding(28895, 1024, padding_idx=0)\n",
      "Weights: torch.Size([28895, 1024]) vs torch.Size([28895, 1024])\n",
      "\n",
      "Comparing Layer: bert.embeddings.position_embeddings -> bert.embeddings.position_embeddings\n",
      "Model 1: Embedding(512, 1024)\n",
      "Model 2: Embedding(512, 1024)\n",
      "Weights: torch.Size([512, 1024]) vs torch.Size([512, 1024])\n",
      "\n",
      "Comparing Layer: bert.embeddings.token_type_embeddings -> bert.embeddings.token_type_embeddings\n",
      "Model 1: Embedding(2, 1024)\n",
      "Model 2: Embedding(2, 1024)\n",
      "Weights: torch.Size([2, 1024]) vs torch.Size([2, 1024])\n",
      "\n",
      "Comparing Layer: bert.embeddings.LayerNorm -> bert.embeddings.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.embeddings.dropout -> bert.embeddings.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder -> bert.encoder\n",
      "Model 1: BertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-23): 24 x BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model 2: BertEncoder(\n",
      "  (layer): ModuleList(\n",
      "    (0-23): 24 x BertLayer(\n",
      "      (attention): BertAttention(\n",
      "        (self): BertSelfAttention(\n",
      "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output): BertSelfOutput(\n",
      "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (intermediate): BertIntermediate(\n",
      "        (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (intermediate_act_fn): GELUActivation()\n",
      "      )\n",
      "      (output): BertOutput(\n",
      "        (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer -> bert.encoder.layer\n",
      "Model 1: ModuleList(\n",
      "  (0-23): 24 x BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model 2: ModuleList(\n",
      "  (0-23): 24 x BertLayer(\n",
      "    (attention): BertAttention(\n",
      "      (self): BertSelfAttention(\n",
      "        (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output): BertSelfOutput(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (intermediate): BertIntermediate(\n",
      "      (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): BertOutput(\n",
      "      (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0 -> bert.encoder.layer.0\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.attention -> bert.encoder.layer.0.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.attention.self -> bert.encoder.layer.0.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.attention.self.query -> bert.encoder.layer.0.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.attention.self.key -> bert.encoder.layer.0.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.attention.self.value -> bert.encoder.layer.0.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.attention.self.dropout -> bert.encoder.layer.0.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.attention.output -> bert.encoder.layer.0.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.attention.output.dense -> bert.encoder.layer.0.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.attention.output.LayerNorm -> bert.encoder.layer.0.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.attention.output.dropout -> bert.encoder.layer.0.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.intermediate -> bert.encoder.layer.0.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.intermediate.dense -> bert.encoder.layer.0.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.intermediate.intermediate_act_fn -> bert.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.output -> bert.encoder.layer.0.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.output.dense -> bert.encoder.layer.0.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.output.LayerNorm -> bert.encoder.layer.0.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.0.output.dropout -> bert.encoder.layer.0.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1 -> bert.encoder.layer.1\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.attention -> bert.encoder.layer.1.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.attention.self -> bert.encoder.layer.1.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.attention.self.query -> bert.encoder.layer.1.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.attention.self.key -> bert.encoder.layer.1.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.attention.self.value -> bert.encoder.layer.1.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.attention.self.dropout -> bert.encoder.layer.1.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.attention.output -> bert.encoder.layer.1.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.attention.output.dense -> bert.encoder.layer.1.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.attention.output.LayerNorm -> bert.encoder.layer.1.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.attention.output.dropout -> bert.encoder.layer.1.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.intermediate -> bert.encoder.layer.1.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.intermediate.dense -> bert.encoder.layer.1.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.intermediate.intermediate_act_fn -> bert.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.output -> bert.encoder.layer.1.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.output.dense -> bert.encoder.layer.1.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.output.LayerNorm -> bert.encoder.layer.1.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.1.output.dropout -> bert.encoder.layer.1.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2 -> bert.encoder.layer.2\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.attention -> bert.encoder.layer.2.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.attention.self -> bert.encoder.layer.2.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.attention.self.query -> bert.encoder.layer.2.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.attention.self.key -> bert.encoder.layer.2.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.attention.self.value -> bert.encoder.layer.2.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.attention.self.dropout -> bert.encoder.layer.2.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.attention.output -> bert.encoder.layer.2.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.attention.output.dense -> bert.encoder.layer.2.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.attention.output.LayerNorm -> bert.encoder.layer.2.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.attention.output.dropout -> bert.encoder.layer.2.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.intermediate -> bert.encoder.layer.2.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.intermediate.dense -> bert.encoder.layer.2.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.intermediate.intermediate_act_fn -> bert.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.output -> bert.encoder.layer.2.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.output.dense -> bert.encoder.layer.2.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.output.LayerNorm -> bert.encoder.layer.2.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.2.output.dropout -> bert.encoder.layer.2.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3 -> bert.encoder.layer.3\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.attention -> bert.encoder.layer.3.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.attention.self -> bert.encoder.layer.3.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.attention.self.query -> bert.encoder.layer.3.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.attention.self.key -> bert.encoder.layer.3.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.attention.self.value -> bert.encoder.layer.3.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.attention.self.dropout -> bert.encoder.layer.3.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.attention.output -> bert.encoder.layer.3.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.attention.output.dense -> bert.encoder.layer.3.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.attention.output.LayerNorm -> bert.encoder.layer.3.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.attention.output.dropout -> bert.encoder.layer.3.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.intermediate -> bert.encoder.layer.3.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.intermediate.dense -> bert.encoder.layer.3.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.intermediate.intermediate_act_fn -> bert.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.output -> bert.encoder.layer.3.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.output.dense -> bert.encoder.layer.3.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.output.LayerNorm -> bert.encoder.layer.3.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.3.output.dropout -> bert.encoder.layer.3.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4 -> bert.encoder.layer.4\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.attention -> bert.encoder.layer.4.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.attention.self -> bert.encoder.layer.4.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.attention.self.query -> bert.encoder.layer.4.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.attention.self.key -> bert.encoder.layer.4.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.attention.self.value -> bert.encoder.layer.4.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.attention.self.dropout -> bert.encoder.layer.4.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.attention.output -> bert.encoder.layer.4.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.attention.output.dense -> bert.encoder.layer.4.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.attention.output.LayerNorm -> bert.encoder.layer.4.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.attention.output.dropout -> bert.encoder.layer.4.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.intermediate -> bert.encoder.layer.4.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.intermediate.dense -> bert.encoder.layer.4.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.intermediate.intermediate_act_fn -> bert.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.output -> bert.encoder.layer.4.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.output.dense -> bert.encoder.layer.4.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.output.LayerNorm -> bert.encoder.layer.4.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.4.output.dropout -> bert.encoder.layer.4.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5 -> bert.encoder.layer.5\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.attention -> bert.encoder.layer.5.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.attention.self -> bert.encoder.layer.5.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.attention.self.query -> bert.encoder.layer.5.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.attention.self.key -> bert.encoder.layer.5.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.attention.self.value -> bert.encoder.layer.5.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.attention.self.dropout -> bert.encoder.layer.5.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.attention.output -> bert.encoder.layer.5.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.attention.output.dense -> bert.encoder.layer.5.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.attention.output.LayerNorm -> bert.encoder.layer.5.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.attention.output.dropout -> bert.encoder.layer.5.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.intermediate -> bert.encoder.layer.5.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.intermediate.dense -> bert.encoder.layer.5.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.intermediate.intermediate_act_fn -> bert.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.output -> bert.encoder.layer.5.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.output.dense -> bert.encoder.layer.5.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.output.LayerNorm -> bert.encoder.layer.5.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.5.output.dropout -> bert.encoder.layer.5.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6 -> bert.encoder.layer.6\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.attention -> bert.encoder.layer.6.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.attention.self -> bert.encoder.layer.6.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.attention.self.query -> bert.encoder.layer.6.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.attention.self.key -> bert.encoder.layer.6.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.attention.self.value -> bert.encoder.layer.6.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.attention.self.dropout -> bert.encoder.layer.6.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.attention.output -> bert.encoder.layer.6.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.attention.output.dense -> bert.encoder.layer.6.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.attention.output.LayerNorm -> bert.encoder.layer.6.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.attention.output.dropout -> bert.encoder.layer.6.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.intermediate -> bert.encoder.layer.6.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.intermediate.dense -> bert.encoder.layer.6.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.intermediate.intermediate_act_fn -> bert.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.output -> bert.encoder.layer.6.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.output.dense -> bert.encoder.layer.6.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.output.LayerNorm -> bert.encoder.layer.6.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.6.output.dropout -> bert.encoder.layer.6.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7 -> bert.encoder.layer.7\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.attention -> bert.encoder.layer.7.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.attention.self -> bert.encoder.layer.7.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.attention.self.query -> bert.encoder.layer.7.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.attention.self.key -> bert.encoder.layer.7.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.attention.self.value -> bert.encoder.layer.7.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.attention.self.dropout -> bert.encoder.layer.7.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.attention.output -> bert.encoder.layer.7.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.attention.output.dense -> bert.encoder.layer.7.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.attention.output.LayerNorm -> bert.encoder.layer.7.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.attention.output.dropout -> bert.encoder.layer.7.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.intermediate -> bert.encoder.layer.7.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.intermediate.dense -> bert.encoder.layer.7.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.intermediate.intermediate_act_fn -> bert.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.output -> bert.encoder.layer.7.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.output.dense -> bert.encoder.layer.7.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.output.LayerNorm -> bert.encoder.layer.7.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.7.output.dropout -> bert.encoder.layer.7.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8 -> bert.encoder.layer.8\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.attention -> bert.encoder.layer.8.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.attention.self -> bert.encoder.layer.8.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.attention.self.query -> bert.encoder.layer.8.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.attention.self.key -> bert.encoder.layer.8.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.attention.self.value -> bert.encoder.layer.8.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.attention.self.dropout -> bert.encoder.layer.8.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.attention.output -> bert.encoder.layer.8.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.attention.output.dense -> bert.encoder.layer.8.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.attention.output.LayerNorm -> bert.encoder.layer.8.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.attention.output.dropout -> bert.encoder.layer.8.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.intermediate -> bert.encoder.layer.8.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.intermediate.dense -> bert.encoder.layer.8.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.intermediate.intermediate_act_fn -> bert.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.output -> bert.encoder.layer.8.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.output.dense -> bert.encoder.layer.8.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.output.LayerNorm -> bert.encoder.layer.8.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.8.output.dropout -> bert.encoder.layer.8.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9 -> bert.encoder.layer.9\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.attention -> bert.encoder.layer.9.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.attention.self -> bert.encoder.layer.9.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.attention.self.query -> bert.encoder.layer.9.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.attention.self.key -> bert.encoder.layer.9.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.attention.self.value -> bert.encoder.layer.9.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.attention.self.dropout -> bert.encoder.layer.9.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.attention.output -> bert.encoder.layer.9.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.attention.output.dense -> bert.encoder.layer.9.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.attention.output.LayerNorm -> bert.encoder.layer.9.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.attention.output.dropout -> bert.encoder.layer.9.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.intermediate -> bert.encoder.layer.9.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.intermediate.dense -> bert.encoder.layer.9.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.intermediate.intermediate_act_fn -> bert.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.output -> bert.encoder.layer.9.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.output.dense -> bert.encoder.layer.9.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.output.LayerNorm -> bert.encoder.layer.9.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.9.output.dropout -> bert.encoder.layer.9.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10 -> bert.encoder.layer.10\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.attention -> bert.encoder.layer.10.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.attention.self -> bert.encoder.layer.10.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.attention.self.query -> bert.encoder.layer.10.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.attention.self.key -> bert.encoder.layer.10.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.attention.self.value -> bert.encoder.layer.10.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.attention.self.dropout -> bert.encoder.layer.10.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.attention.output -> bert.encoder.layer.10.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.attention.output.dense -> bert.encoder.layer.10.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.attention.output.LayerNorm -> bert.encoder.layer.10.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.attention.output.dropout -> bert.encoder.layer.10.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.intermediate -> bert.encoder.layer.10.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.intermediate.dense -> bert.encoder.layer.10.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.intermediate.intermediate_act_fn -> bert.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.output -> bert.encoder.layer.10.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.output.dense -> bert.encoder.layer.10.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.output.LayerNorm -> bert.encoder.layer.10.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.10.output.dropout -> bert.encoder.layer.10.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11 -> bert.encoder.layer.11\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.attention -> bert.encoder.layer.11.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.attention.self -> bert.encoder.layer.11.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.attention.self.query -> bert.encoder.layer.11.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.attention.self.key -> bert.encoder.layer.11.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.attention.self.value -> bert.encoder.layer.11.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.attention.self.dropout -> bert.encoder.layer.11.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.attention.output -> bert.encoder.layer.11.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.attention.output.dense -> bert.encoder.layer.11.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.attention.output.LayerNorm -> bert.encoder.layer.11.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.attention.output.dropout -> bert.encoder.layer.11.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.intermediate -> bert.encoder.layer.11.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.intermediate.dense -> bert.encoder.layer.11.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.intermediate.intermediate_act_fn -> bert.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.output -> bert.encoder.layer.11.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.output.dense -> bert.encoder.layer.11.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.output.LayerNorm -> bert.encoder.layer.11.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.11.output.dropout -> bert.encoder.layer.11.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12 -> bert.encoder.layer.12\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.attention -> bert.encoder.layer.12.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.attention.self -> bert.encoder.layer.12.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.attention.self.query -> bert.encoder.layer.12.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.attention.self.key -> bert.encoder.layer.12.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.attention.self.value -> bert.encoder.layer.12.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.attention.self.dropout -> bert.encoder.layer.12.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.attention.output -> bert.encoder.layer.12.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.attention.output.dense -> bert.encoder.layer.12.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.attention.output.LayerNorm -> bert.encoder.layer.12.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.attention.output.dropout -> bert.encoder.layer.12.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.intermediate -> bert.encoder.layer.12.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.intermediate.dense -> bert.encoder.layer.12.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.intermediate.intermediate_act_fn -> bert.encoder.layer.12.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.output -> bert.encoder.layer.12.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.output.dense -> bert.encoder.layer.12.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.output.LayerNorm -> bert.encoder.layer.12.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.12.output.dropout -> bert.encoder.layer.12.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13 -> bert.encoder.layer.13\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.attention -> bert.encoder.layer.13.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.attention.self -> bert.encoder.layer.13.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.attention.self.query -> bert.encoder.layer.13.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.attention.self.key -> bert.encoder.layer.13.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.attention.self.value -> bert.encoder.layer.13.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.attention.self.dropout -> bert.encoder.layer.13.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.attention.output -> bert.encoder.layer.13.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.attention.output.dense -> bert.encoder.layer.13.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.attention.output.LayerNorm -> bert.encoder.layer.13.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.attention.output.dropout -> bert.encoder.layer.13.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.intermediate -> bert.encoder.layer.13.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.intermediate.dense -> bert.encoder.layer.13.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.intermediate.intermediate_act_fn -> bert.encoder.layer.13.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.output -> bert.encoder.layer.13.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.output.dense -> bert.encoder.layer.13.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.output.LayerNorm -> bert.encoder.layer.13.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.13.output.dropout -> bert.encoder.layer.13.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14 -> bert.encoder.layer.14\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.attention -> bert.encoder.layer.14.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.attention.self -> bert.encoder.layer.14.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.attention.self.query -> bert.encoder.layer.14.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.attention.self.key -> bert.encoder.layer.14.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.attention.self.value -> bert.encoder.layer.14.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.attention.self.dropout -> bert.encoder.layer.14.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.attention.output -> bert.encoder.layer.14.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.attention.output.dense -> bert.encoder.layer.14.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.attention.output.LayerNorm -> bert.encoder.layer.14.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.attention.output.dropout -> bert.encoder.layer.14.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.intermediate -> bert.encoder.layer.14.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.intermediate.dense -> bert.encoder.layer.14.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.intermediate.intermediate_act_fn -> bert.encoder.layer.14.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.output -> bert.encoder.layer.14.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.output.dense -> bert.encoder.layer.14.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.output.LayerNorm -> bert.encoder.layer.14.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.14.output.dropout -> bert.encoder.layer.14.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15 -> bert.encoder.layer.15\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.attention -> bert.encoder.layer.15.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.attention.self -> bert.encoder.layer.15.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.attention.self.query -> bert.encoder.layer.15.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.attention.self.key -> bert.encoder.layer.15.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.attention.self.value -> bert.encoder.layer.15.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.attention.self.dropout -> bert.encoder.layer.15.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.attention.output -> bert.encoder.layer.15.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.attention.output.dense -> bert.encoder.layer.15.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.attention.output.LayerNorm -> bert.encoder.layer.15.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.attention.output.dropout -> bert.encoder.layer.15.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.intermediate -> bert.encoder.layer.15.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.intermediate.dense -> bert.encoder.layer.15.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.intermediate.intermediate_act_fn -> bert.encoder.layer.15.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.output -> bert.encoder.layer.15.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.output.dense -> bert.encoder.layer.15.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.output.LayerNorm -> bert.encoder.layer.15.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.15.output.dropout -> bert.encoder.layer.15.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16 -> bert.encoder.layer.16\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.attention -> bert.encoder.layer.16.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.attention.self -> bert.encoder.layer.16.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.attention.self.query -> bert.encoder.layer.16.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.attention.self.key -> bert.encoder.layer.16.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.attention.self.value -> bert.encoder.layer.16.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.attention.self.dropout -> bert.encoder.layer.16.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.attention.output -> bert.encoder.layer.16.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.attention.output.dense -> bert.encoder.layer.16.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.attention.output.LayerNorm -> bert.encoder.layer.16.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.attention.output.dropout -> bert.encoder.layer.16.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.intermediate -> bert.encoder.layer.16.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.intermediate.dense -> bert.encoder.layer.16.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.intermediate.intermediate_act_fn -> bert.encoder.layer.16.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.output -> bert.encoder.layer.16.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.output.dense -> bert.encoder.layer.16.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.output.LayerNorm -> bert.encoder.layer.16.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.16.output.dropout -> bert.encoder.layer.16.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17 -> bert.encoder.layer.17\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.attention -> bert.encoder.layer.17.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.attention.self -> bert.encoder.layer.17.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.attention.self.query -> bert.encoder.layer.17.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.attention.self.key -> bert.encoder.layer.17.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.attention.self.value -> bert.encoder.layer.17.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.attention.self.dropout -> bert.encoder.layer.17.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.attention.output -> bert.encoder.layer.17.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.attention.output.dense -> bert.encoder.layer.17.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.attention.output.LayerNorm -> bert.encoder.layer.17.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.attention.output.dropout -> bert.encoder.layer.17.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.intermediate -> bert.encoder.layer.17.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.intermediate.dense -> bert.encoder.layer.17.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.intermediate.intermediate_act_fn -> bert.encoder.layer.17.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.output -> bert.encoder.layer.17.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.output.dense -> bert.encoder.layer.17.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.output.LayerNorm -> bert.encoder.layer.17.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.17.output.dropout -> bert.encoder.layer.17.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18 -> bert.encoder.layer.18\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.attention -> bert.encoder.layer.18.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.attention.self -> bert.encoder.layer.18.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.attention.self.query -> bert.encoder.layer.18.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.attention.self.key -> bert.encoder.layer.18.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.attention.self.value -> bert.encoder.layer.18.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.attention.self.dropout -> bert.encoder.layer.18.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.attention.output -> bert.encoder.layer.18.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.attention.output.dense -> bert.encoder.layer.18.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.attention.output.LayerNorm -> bert.encoder.layer.18.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.attention.output.dropout -> bert.encoder.layer.18.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.intermediate -> bert.encoder.layer.18.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.intermediate.dense -> bert.encoder.layer.18.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.intermediate.intermediate_act_fn -> bert.encoder.layer.18.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.output -> bert.encoder.layer.18.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.output.dense -> bert.encoder.layer.18.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.output.LayerNorm -> bert.encoder.layer.18.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.18.output.dropout -> bert.encoder.layer.18.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19 -> bert.encoder.layer.19\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.attention -> bert.encoder.layer.19.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.attention.self -> bert.encoder.layer.19.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.attention.self.query -> bert.encoder.layer.19.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.attention.self.key -> bert.encoder.layer.19.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.attention.self.value -> bert.encoder.layer.19.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.attention.self.dropout -> bert.encoder.layer.19.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.attention.output -> bert.encoder.layer.19.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.attention.output.dense -> bert.encoder.layer.19.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.attention.output.LayerNorm -> bert.encoder.layer.19.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.attention.output.dropout -> bert.encoder.layer.19.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.intermediate -> bert.encoder.layer.19.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.intermediate.dense -> bert.encoder.layer.19.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.intermediate.intermediate_act_fn -> bert.encoder.layer.19.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.output -> bert.encoder.layer.19.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.output.dense -> bert.encoder.layer.19.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.output.LayerNorm -> bert.encoder.layer.19.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.19.output.dropout -> bert.encoder.layer.19.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20 -> bert.encoder.layer.20\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.attention -> bert.encoder.layer.20.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.attention.self -> bert.encoder.layer.20.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.attention.self.query -> bert.encoder.layer.20.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.attention.self.key -> bert.encoder.layer.20.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.attention.self.value -> bert.encoder.layer.20.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.attention.self.dropout -> bert.encoder.layer.20.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.attention.output -> bert.encoder.layer.20.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.attention.output.dense -> bert.encoder.layer.20.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.attention.output.LayerNorm -> bert.encoder.layer.20.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.attention.output.dropout -> bert.encoder.layer.20.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.intermediate -> bert.encoder.layer.20.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.intermediate.dense -> bert.encoder.layer.20.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.intermediate.intermediate_act_fn -> bert.encoder.layer.20.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.output -> bert.encoder.layer.20.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.output.dense -> bert.encoder.layer.20.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.output.LayerNorm -> bert.encoder.layer.20.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.20.output.dropout -> bert.encoder.layer.20.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21 -> bert.encoder.layer.21\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.attention -> bert.encoder.layer.21.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.attention.self -> bert.encoder.layer.21.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.attention.self.query -> bert.encoder.layer.21.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.attention.self.key -> bert.encoder.layer.21.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.attention.self.value -> bert.encoder.layer.21.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.attention.self.dropout -> bert.encoder.layer.21.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.attention.output -> bert.encoder.layer.21.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.attention.output.dense -> bert.encoder.layer.21.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.attention.output.LayerNorm -> bert.encoder.layer.21.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.attention.output.dropout -> bert.encoder.layer.21.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.intermediate -> bert.encoder.layer.21.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.intermediate.dense -> bert.encoder.layer.21.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.intermediate.intermediate_act_fn -> bert.encoder.layer.21.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.output -> bert.encoder.layer.21.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.output.dense -> bert.encoder.layer.21.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.output.LayerNorm -> bert.encoder.layer.21.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.21.output.dropout -> bert.encoder.layer.21.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22 -> bert.encoder.layer.22\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.attention -> bert.encoder.layer.22.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.attention.self -> bert.encoder.layer.22.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.attention.self.query -> bert.encoder.layer.22.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.attention.self.key -> bert.encoder.layer.22.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.attention.self.value -> bert.encoder.layer.22.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.attention.self.dropout -> bert.encoder.layer.22.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.attention.output -> bert.encoder.layer.22.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.attention.output.dense -> bert.encoder.layer.22.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.attention.output.LayerNorm -> bert.encoder.layer.22.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.attention.output.dropout -> bert.encoder.layer.22.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.intermediate -> bert.encoder.layer.22.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.intermediate.dense -> bert.encoder.layer.22.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.intermediate.intermediate_act_fn -> bert.encoder.layer.22.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.output -> bert.encoder.layer.22.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.output.dense -> bert.encoder.layer.22.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.output.LayerNorm -> bert.encoder.layer.22.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.22.output.dropout -> bert.encoder.layer.22.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23 -> bert.encoder.layer.23\n",
      "Model 1: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.attention -> bert.encoder.layer.23.attention\n",
      "Model 1: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Model 2: BertAttention(\n",
      "  (self): BertSelfAttention(\n",
      "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (output): BertSelfOutput(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.attention.self -> bert.encoder.layer.23.attention.self\n",
      "Model 1: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfAttention(\n",
      "  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.attention.self.query -> bert.encoder.layer.23.attention.self.query\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.attention.self.key -> bert.encoder.layer.23.attention.self.key\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.attention.self.value -> bert.encoder.layer.23.attention.self.value\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.attention.self.dropout -> bert.encoder.layer.23.attention.self.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.attention.output -> bert.encoder.layer.23.attention.output\n",
      "Model 1: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertSelfOutput(\n",
      "  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.attention.output.dense -> bert.encoder.layer.23.attention.output.dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.attention.output.LayerNorm -> bert.encoder.layer.23.attention.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.attention.output.dropout -> bert.encoder.layer.23.attention.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.intermediate -> bert.encoder.layer.23.intermediate\n",
      "Model 1: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "Model 2: BertIntermediate(\n",
      "  (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (intermediate_act_fn): GELUActivation()\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.intermediate.dense -> bert.encoder.layer.23.intermediate.dense\n",
      "Model 1: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=4096, bias=True)\n",
      "Weights: torch.Size([4096, 1024]) vs torch.Size([4096, 1024])\n",
      "Biases: torch.Size([4096]) vs torch.Size([4096])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.intermediate.intermediate_act_fn -> bert.encoder.layer.23.intermediate.intermediate_act_fn\n",
      "Model 1: GELUActivation()\n",
      "Model 2: GELUActivation()\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.output -> bert.encoder.layer.23.output\n",
      "Model 1: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Model 2: BertOutput(\n",
      "  (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.output.dense -> bert.encoder.layer.23.output.dense\n",
      "Model 1: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=4096, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 4096]) vs torch.Size([1024, 4096])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.output.LayerNorm -> bert.encoder.layer.23.output.LayerNorm\n",
      "Model 1: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Model 2: LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "Weights: torch.Size([1024]) vs torch.Size([1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: bert.encoder.layer.23.output.dropout -> bert.encoder.layer.23.output.dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: dropout -> dropout\n",
      "Model 1: Dropout(p=0.1, inplace=False)\n",
      "Model 2: Dropout(p=0.1, inplace=False)\n",
      "\n",
      "Comparing Layer: dense -> dense\n",
      "Model 1: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=1024, bias=True)\n",
      "Weights: torch.Size([1024, 1024]) vs torch.Size([1024, 1024])\n",
      "Biases: torch.Size([1024]) vs torch.Size([1024])\n",
      "\n",
      "Comparing Layer: dense_activation -> dense_activation\n",
      "Model 1: GELU(approximate='none')\n",
      "Model 2: GELU(approximate='none')\n",
      "\n",
      "Comparing Layer: classifier -> classifier\n",
      "Model 1: Linear(in_features=1024, out_features=13, bias=True)\n",
      "Model 2: Linear(in_features=1024, out_features=13, bias=True)\n",
      "Weights: torch.Size([13, 1024]) vs torch.Size([13, 1024])\n",
      "Biases: torch.Size([13]) vs torch.Size([13])\n",
      "\n",
      "Comparing Layer: crf -> crf\n",
      "Model 1: CRF(num_tags=13)\n",
      "Model 2: CRF(num_tags=13)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_models(model, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_weight_magnitudes(model1, model2):\n",
    "    for (name1, module1), (name2, module2) in zip(model1.named_modules(), model2.named_modules()):\n",
    "        if hasattr(module1, 'weight') and hasattr(module2, 'weight'):\n",
    "            norm1 = torch.norm(module1.weight, p='fro').item()\n",
    "            norm2 = torch.norm(module2.weight, p='fro').item()\n",
    "            if norm1 != norm2:\n",
    "                print(f\"Layer: {name1}\")\n",
    "                print(f\"Model 1 Weight Magnitude (Frobenius norm): {norm1}\")\n",
    "                print(f\"Model 2 Weight Magnitude (Frobenius norm): {norm2}\")\n",
    "                print(f\"Difference in Magnitudes: {abs(norm1 - norm2)}\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: bert.embeddings.word_embeddings\n",
      "Model 1 Weight Magnitude (Frobenius norm): 375.8812255859375\n",
      "Model 2 Weight Magnitude (Frobenius norm): 375.8370361328125\n",
      "Difference in Magnitudes: 0.044189453125\n",
      "\n",
      "Layer: bert.embeddings.position_embeddings\n",
      "Model 1 Weight Magnitude (Frobenius norm): 22.521188735961914\n",
      "Model 2 Weight Magnitude (Frobenius norm): 22.51564598083496\n",
      "Difference in Magnitudes: 0.005542755126953125\n",
      "\n",
      "Layer: bert.embeddings.token_type_embeddings\n",
      "Model 1 Weight Magnitude (Frobenius norm): 1.7138431072235107\n",
      "Model 2 Weight Magnitude (Frobenius norm): 1.712285041809082\n",
      "Difference in Magnitudes: 0.001558065414428711\n",
      "\n",
      "Layer: bert.embeddings.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 16.448768615722656\n",
      "Model 2 Weight Magnitude (Frobenius norm): 16.450185775756836\n",
      "Difference in Magnitudes: 0.0014171600341796875\n",
      "\n",
      "Layer: bert.encoder.layer.0.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 50.29220199584961\n",
      "Model 2 Weight Magnitude (Frobenius norm): 50.29047775268555\n",
      "Difference in Magnitudes: 0.0017242431640625\n",
      "\n",
      "Layer: bert.encoder.layer.0.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 48.4567985534668\n",
      "Model 2 Weight Magnitude (Frobenius norm): 48.456382751464844\n",
      "Difference in Magnitudes: 0.000415802001953125\n",
      "\n",
      "Layer: bert.encoder.layer.0.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 32.20892333984375\n",
      "Model 2 Weight Magnitude (Frobenius norm): 32.20536422729492\n",
      "Difference in Magnitudes: 0.003559112548828125\n",
      "\n",
      "Layer: bert.encoder.layer.0.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 30.868375778198242\n",
      "Model 2 Weight Magnitude (Frobenius norm): 30.864639282226562\n",
      "Difference in Magnitudes: 0.0037364959716796875\n",
      "\n",
      "Layer: bert.encoder.layer.0.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 26.45686912536621\n",
      "Model 2 Weight Magnitude (Frobenius norm): 26.458335876464844\n",
      "Difference in Magnitudes: 0.0014667510986328125\n",
      "\n",
      "Layer: bert.encoder.layer.0.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 94.66792297363281\n",
      "Model 2 Weight Magnitude (Frobenius norm): 94.66621398925781\n",
      "Difference in Magnitudes: 0.001708984375\n",
      "\n",
      "Layer: bert.encoder.layer.0.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 85.4244384765625\n",
      "Model 2 Weight Magnitude (Frobenius norm): 85.42149353027344\n",
      "Difference in Magnitudes: 0.0029449462890625\n",
      "\n",
      "Layer: bert.encoder.layer.0.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 17.575809478759766\n",
      "Model 2 Weight Magnitude (Frobenius norm): 17.5742130279541\n",
      "Difference in Magnitudes: 0.0015964508056640625\n",
      "\n",
      "Layer: bert.encoder.layer.1.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 54.19244384765625\n",
      "Model 2 Weight Magnitude (Frobenius norm): 54.18440246582031\n",
      "Difference in Magnitudes: 0.0080413818359375\n",
      "\n",
      "Layer: bert.encoder.layer.1.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 52.80424880981445\n",
      "Model 2 Weight Magnitude (Frobenius norm): 52.7960319519043\n",
      "Difference in Magnitudes: 0.00821685791015625\n",
      "\n",
      "Layer: bert.encoder.layer.1.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 34.149024963378906\n",
      "Model 2 Weight Magnitude (Frobenius norm): 34.141754150390625\n",
      "Difference in Magnitudes: 0.00727081298828125\n",
      "\n",
      "Layer: bert.encoder.layer.1.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 33.393985748291016\n",
      "Model 2 Weight Magnitude (Frobenius norm): 33.386688232421875\n",
      "Difference in Magnitudes: 0.007297515869140625\n",
      "\n",
      "Layer: bert.encoder.layer.1.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 25.142709732055664\n",
      "Model 2 Weight Magnitude (Frobenius norm): 25.142566680908203\n",
      "Difference in Magnitudes: 0.0001430511474609375\n",
      "\n",
      "Layer: bert.encoder.layer.1.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 104.95256805419922\n",
      "Model 2 Weight Magnitude (Frobenius norm): 104.94466400146484\n",
      "Difference in Magnitudes: 0.007904052734375\n",
      "\n",
      "Layer: bert.encoder.layer.1.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 84.62184143066406\n",
      "Model 2 Weight Magnitude (Frobenius norm): 84.61949157714844\n",
      "Difference in Magnitudes: 0.002349853515625\n",
      "\n",
      "Layer: bert.encoder.layer.1.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.247671127319336\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.247600555419922\n",
      "Difference in Magnitudes: 7.05718994140625e-05\n",
      "\n",
      "Layer: bert.encoder.layer.2.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 51.2918815612793\n",
      "Model 2 Weight Magnitude (Frobenius norm): 51.28851318359375\n",
      "Difference in Magnitudes: 0.003368377685546875\n",
      "\n",
      "Layer: bert.encoder.layer.2.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 49.67774200439453\n",
      "Model 2 Weight Magnitude (Frobenius norm): 49.67417907714844\n",
      "Difference in Magnitudes: 0.00356292724609375\n",
      "\n",
      "Layer: bert.encoder.layer.2.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 37.68475341796875\n",
      "Model 2 Weight Magnitude (Frobenius norm): 37.679039001464844\n",
      "Difference in Magnitudes: 0.00571441650390625\n",
      "\n",
      "Layer: bert.encoder.layer.2.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 35.22242736816406\n",
      "Model 2 Weight Magnitude (Frobenius norm): 35.2165412902832\n",
      "Difference in Magnitudes: 0.005886077880859375\n",
      "\n",
      "Layer: bert.encoder.layer.2.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 23.992815017700195\n",
      "Model 2 Weight Magnitude (Frobenius norm): 23.993255615234375\n",
      "Difference in Magnitudes: 0.0004405975341796875\n",
      "\n",
      "Layer: bert.encoder.layer.2.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 104.93084716796875\n",
      "Model 2 Weight Magnitude (Frobenius norm): 104.92408752441406\n",
      "Difference in Magnitudes: 0.0067596435546875\n",
      "\n",
      "Layer: bert.encoder.layer.2.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 86.4820327758789\n",
      "Model 2 Weight Magnitude (Frobenius norm): 86.47881317138672\n",
      "Difference in Magnitudes: 0.0032196044921875\n",
      "\n",
      "Layer: bert.encoder.layer.2.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 19.27269172668457\n",
      "Model 2 Weight Magnitude (Frobenius norm): 19.27345085144043\n",
      "Difference in Magnitudes: 0.000759124755859375\n",
      "\n",
      "Layer: bert.encoder.layer.3.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 52.602569580078125\n",
      "Model 2 Weight Magnitude (Frobenius norm): 52.60091781616211\n",
      "Difference in Magnitudes: 0.001651763916015625\n",
      "\n",
      "Layer: bert.encoder.layer.3.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 48.9095344543457\n",
      "Model 2 Weight Magnitude (Frobenius norm): 48.90721893310547\n",
      "Difference in Magnitudes: 0.002315521240234375\n",
      "\n",
      "Layer: bert.encoder.layer.3.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 33.8787727355957\n",
      "Model 2 Weight Magnitude (Frobenius norm): 33.874656677246094\n",
      "Difference in Magnitudes: 0.004116058349609375\n",
      "\n",
      "Layer: bert.encoder.layer.3.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 33.0185432434082\n",
      "Model 2 Weight Magnitude (Frobenius norm): 33.01421356201172\n",
      "Difference in Magnitudes: 0.004329681396484375\n",
      "\n",
      "Layer: bert.encoder.layer.3.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 23.392168045043945\n",
      "Model 2 Weight Magnitude (Frobenius norm): 23.392562866210938\n",
      "Difference in Magnitudes: 0.0003948211669921875\n",
      "\n",
      "Layer: bert.encoder.layer.3.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 104.96881866455078\n",
      "Model 2 Weight Magnitude (Frobenius norm): 104.96241760253906\n",
      "Difference in Magnitudes: 0.00640106201171875\n",
      "\n",
      "Layer: bert.encoder.layer.3.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 88.32254028320312\n",
      "Model 2 Weight Magnitude (Frobenius norm): 88.32096099853516\n",
      "Difference in Magnitudes: 0.00157928466796875\n",
      "\n",
      "Layer: bert.encoder.layer.3.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.676544189453125\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.67576026916504\n",
      "Difference in Magnitudes: 0.0007839202880859375\n",
      "\n",
      "Layer: bert.encoder.layer.4.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 52.9678955078125\n",
      "Model 2 Weight Magnitude (Frobenius norm): 52.96232604980469\n",
      "Difference in Magnitudes: 0.0055694580078125\n",
      "\n",
      "Layer: bert.encoder.layer.4.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 50.374935150146484\n",
      "Model 2 Weight Magnitude (Frobenius norm): 50.37118911743164\n",
      "Difference in Magnitudes: 0.00374603271484375\n",
      "\n",
      "Layer: bert.encoder.layer.4.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 34.17118835449219\n",
      "Model 2 Weight Magnitude (Frobenius norm): 34.166011810302734\n",
      "Difference in Magnitudes: 0.005176544189453125\n",
      "\n",
      "Layer: bert.encoder.layer.4.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 33.569156646728516\n",
      "Model 2 Weight Magnitude (Frobenius norm): 33.5638427734375\n",
      "Difference in Magnitudes: 0.005313873291015625\n",
      "\n",
      "Layer: bert.encoder.layer.4.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 22.322174072265625\n",
      "Model 2 Weight Magnitude (Frobenius norm): 22.32119369506836\n",
      "Difference in Magnitudes: 0.000980377197265625\n",
      "\n",
      "Layer: bert.encoder.layer.4.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 103.56110382080078\n",
      "Model 2 Weight Magnitude (Frobenius norm): 103.55303955078125\n",
      "Difference in Magnitudes: 0.00806427001953125\n",
      "\n",
      "Layer: bert.encoder.layer.4.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 88.54448699951172\n",
      "Model 2 Weight Magnitude (Frobenius norm): 88.54194641113281\n",
      "Difference in Magnitudes: 0.00254058837890625\n",
      "\n",
      "Layer: bert.encoder.layer.4.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 17.39894676208496\n",
      "Model 2 Weight Magnitude (Frobenius norm): 17.39793586730957\n",
      "Difference in Magnitudes: 0.001010894775390625\n",
      "\n",
      "Layer: bert.encoder.layer.5.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 52.21949005126953\n",
      "Model 2 Weight Magnitude (Frobenius norm): 52.211181640625\n",
      "Difference in Magnitudes: 0.00830841064453125\n",
      "\n",
      "Layer: bert.encoder.layer.5.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 51.23893356323242\n",
      "Model 2 Weight Magnitude (Frobenius norm): 51.2294807434082\n",
      "Difference in Magnitudes: 0.00945281982421875\n",
      "\n",
      "Layer: bert.encoder.layer.5.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 33.82427215576172\n",
      "Model 2 Weight Magnitude (Frobenius norm): 33.819068908691406\n",
      "Difference in Magnitudes: 0.0052032470703125\n",
      "\n",
      "Layer: bert.encoder.layer.5.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 33.92104721069336\n",
      "Model 2 Weight Magnitude (Frobenius norm): 33.91529083251953\n",
      "Difference in Magnitudes: 0.005756378173828125\n",
      "\n",
      "Layer: bert.encoder.layer.5.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 22.923200607299805\n",
      "Model 2 Weight Magnitude (Frobenius norm): 22.92337989807129\n",
      "Difference in Magnitudes: 0.000179290771484375\n",
      "\n",
      "Layer: bert.encoder.layer.5.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 102.81458282470703\n",
      "Model 2 Weight Magnitude (Frobenius norm): 102.80799865722656\n",
      "Difference in Magnitudes: 0.00658416748046875\n",
      "\n",
      "Layer: bert.encoder.layer.5.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 88.51863861083984\n",
      "Model 2 Weight Magnitude (Frobenius norm): 88.51890563964844\n",
      "Difference in Magnitudes: 0.00026702880859375\n",
      "\n",
      "Layer: bert.encoder.layer.5.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 17.971923828125\n",
      "Model 2 Weight Magnitude (Frobenius norm): 17.971729278564453\n",
      "Difference in Magnitudes: 0.000194549560546875\n",
      "\n",
      "Layer: bert.encoder.layer.6.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 52.34219741821289\n",
      "Model 2 Weight Magnitude (Frobenius norm): 52.33692932128906\n",
      "Difference in Magnitudes: 0.005268096923828125\n",
      "\n",
      "Layer: bert.encoder.layer.6.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 51.16193771362305\n",
      "Model 2 Weight Magnitude (Frobenius norm): 51.15586471557617\n",
      "Difference in Magnitudes: 0.006072998046875\n",
      "\n",
      "Layer: bert.encoder.layer.6.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 34.430789947509766\n",
      "Model 2 Weight Magnitude (Frobenius norm): 34.42609405517578\n",
      "Difference in Magnitudes: 0.004695892333984375\n",
      "\n",
      "Layer: bert.encoder.layer.6.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 33.42377471923828\n",
      "Model 2 Weight Magnitude (Frobenius norm): 33.41878128051758\n",
      "Difference in Magnitudes: 0.004993438720703125\n",
      "\n",
      "Layer: bert.encoder.layer.6.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 21.95529556274414\n",
      "Model 2 Weight Magnitude (Frobenius norm): 21.955812454223633\n",
      "Difference in Magnitudes: 0.0005168914794921875\n",
      "\n",
      "Layer: bert.encoder.layer.6.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 101.1581802368164\n",
      "Model 2 Weight Magnitude (Frobenius norm): 101.1507797241211\n",
      "Difference in Magnitudes: 0.0074005126953125\n",
      "\n",
      "Layer: bert.encoder.layer.6.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 86.36933135986328\n",
      "Model 2 Weight Magnitude (Frobenius norm): 86.36646270751953\n",
      "Difference in Magnitudes: 0.00286865234375\n",
      "\n",
      "Layer: bert.encoder.layer.6.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.34915542602539\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.349435806274414\n",
      "Difference in Magnitudes: 0.0002803802490234375\n",
      "\n",
      "Layer: bert.encoder.layer.7.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 55.46488952636719\n",
      "Model 2 Weight Magnitude (Frobenius norm): 55.45765686035156\n",
      "Difference in Magnitudes: 0.007232666015625\n",
      "\n",
      "Layer: bert.encoder.layer.7.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 54.295162200927734\n",
      "Model 2 Weight Magnitude (Frobenius norm): 54.289249420166016\n",
      "Difference in Magnitudes: 0.00591278076171875\n",
      "\n",
      "Layer: bert.encoder.layer.7.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 32.93147277832031\n",
      "Model 2 Weight Magnitude (Frobenius norm): 32.92826461791992\n",
      "Difference in Magnitudes: 0.003208160400390625\n",
      "\n",
      "Layer: bert.encoder.layer.7.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 31.95487403869629\n",
      "Model 2 Weight Magnitude (Frobenius norm): 31.951496124267578\n",
      "Difference in Magnitudes: 0.0033779144287109375\n",
      "\n",
      "Layer: bert.encoder.layer.7.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 21.174467086791992\n",
      "Model 2 Weight Magnitude (Frobenius norm): 21.176315307617188\n",
      "Difference in Magnitudes: 0.0018482208251953125\n",
      "\n",
      "Layer: bert.encoder.layer.7.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 99.3414077758789\n",
      "Model 2 Weight Magnitude (Frobenius norm): 99.33609008789062\n",
      "Difference in Magnitudes: 0.00531768798828125\n",
      "\n",
      "Layer: bert.encoder.layer.7.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 85.69230651855469\n",
      "Model 2 Weight Magnitude (Frobenius norm): 85.6894760131836\n",
      "Difference in Magnitudes: 0.00283050537109375\n",
      "\n",
      "Layer: bert.encoder.layer.7.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.693058013916016\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.692777633666992\n",
      "Difference in Magnitudes: 0.0002803802490234375\n",
      "\n",
      "Layer: bert.encoder.layer.8.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 55.744686126708984\n",
      "Model 2 Weight Magnitude (Frobenius norm): 55.73524856567383\n",
      "Difference in Magnitudes: 0.00943756103515625\n",
      "\n",
      "Layer: bert.encoder.layer.8.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 55.8609733581543\n",
      "Model 2 Weight Magnitude (Frobenius norm): 55.853515625\n",
      "Difference in Magnitudes: 0.007457733154296875\n",
      "\n",
      "Layer: bert.encoder.layer.8.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 35.60215759277344\n",
      "Model 2 Weight Magnitude (Frobenius norm): 35.597347259521484\n",
      "Difference in Magnitudes: 0.004810333251953125\n",
      "\n",
      "Layer: bert.encoder.layer.8.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 35.04949188232422\n",
      "Model 2 Weight Magnitude (Frobenius norm): 35.044193267822266\n",
      "Difference in Magnitudes: 0.005298614501953125\n",
      "\n",
      "Layer: bert.encoder.layer.8.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 19.321269989013672\n",
      "Model 2 Weight Magnitude (Frobenius norm): 19.322999954223633\n",
      "Difference in Magnitudes: 0.0017299652099609375\n",
      "\n",
      "Layer: bert.encoder.layer.8.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 97.85822296142578\n",
      "Model 2 Weight Magnitude (Frobenius norm): 97.85563659667969\n",
      "Difference in Magnitudes: 0.00258636474609375\n",
      "\n",
      "Layer: bert.encoder.layer.8.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 85.22289276123047\n",
      "Model 2 Weight Magnitude (Frobenius norm): 85.21873474121094\n",
      "Difference in Magnitudes: 0.00415802001953125\n",
      "\n",
      "Layer: bert.encoder.layer.8.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 19.303491592407227\n",
      "Model 2 Weight Magnitude (Frobenius norm): 19.303482055664062\n",
      "Difference in Magnitudes: 9.5367431640625e-06\n",
      "\n",
      "Layer: bert.encoder.layer.9.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 54.81932830810547\n",
      "Model 2 Weight Magnitude (Frobenius norm): 54.81488037109375\n",
      "Difference in Magnitudes: 0.00444793701171875\n",
      "\n",
      "Layer: bert.encoder.layer.9.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 54.36377716064453\n",
      "Model 2 Weight Magnitude (Frobenius norm): 54.36005401611328\n",
      "Difference in Magnitudes: 0.00372314453125\n",
      "\n",
      "Layer: bert.encoder.layer.9.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 35.28242111206055\n",
      "Model 2 Weight Magnitude (Frobenius norm): 35.279361724853516\n",
      "Difference in Magnitudes: 0.00305938720703125\n",
      "\n",
      "Layer: bert.encoder.layer.9.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 34.570560455322266\n",
      "Model 2 Weight Magnitude (Frobenius norm): 34.56686782836914\n",
      "Difference in Magnitudes: 0.003692626953125\n",
      "\n",
      "Layer: bert.encoder.layer.9.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.258956909179688\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.259050369262695\n",
      "Difference in Magnitudes: 9.34600830078125e-05\n",
      "\n",
      "Layer: bert.encoder.layer.9.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 97.65032196044922\n",
      "Model 2 Weight Magnitude (Frobenius norm): 97.64602661132812\n",
      "Difference in Magnitudes: 0.00429534912109375\n",
      "\n",
      "Layer: bert.encoder.layer.9.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 84.59736633300781\n",
      "Model 2 Weight Magnitude (Frobenius norm): 84.59346008300781\n",
      "Difference in Magnitudes: 0.00390625\n",
      "\n",
      "Layer: bert.encoder.layer.9.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 19.63937759399414\n",
      "Model 2 Weight Magnitude (Frobenius norm): 19.638748168945312\n",
      "Difference in Magnitudes: 0.000629425048828125\n",
      "\n",
      "Layer: bert.encoder.layer.10.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 54.90278244018555\n",
      "Model 2 Weight Magnitude (Frobenius norm): 54.896636962890625\n",
      "Difference in Magnitudes: 0.006145477294921875\n",
      "\n",
      "Layer: bert.encoder.layer.10.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 54.46880340576172\n",
      "Model 2 Weight Magnitude (Frobenius norm): 54.46305847167969\n",
      "Difference in Magnitudes: 0.00574493408203125\n",
      "\n",
      "Layer: bert.encoder.layer.10.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 33.47761154174805\n",
      "Model 2 Weight Magnitude (Frobenius norm): 33.474849700927734\n",
      "Difference in Magnitudes: 0.0027618408203125\n",
      "\n",
      "Layer: bert.encoder.layer.10.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 32.94261932373047\n",
      "Model 2 Weight Magnitude (Frobenius norm): 32.93978500366211\n",
      "Difference in Magnitudes: 0.002834320068359375\n",
      "\n",
      "Layer: bert.encoder.layer.10.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.404394149780273\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.40426254272461\n",
      "Difference in Magnitudes: 0.0001316070556640625\n",
      "\n",
      "Layer: bert.encoder.layer.10.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 97.12320709228516\n",
      "Model 2 Weight Magnitude (Frobenius norm): 97.11772155761719\n",
      "Difference in Magnitudes: 0.00548553466796875\n",
      "\n",
      "Layer: bert.encoder.layer.10.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 83.3134765625\n",
      "Model 2 Weight Magnitude (Frobenius norm): 83.30989074707031\n",
      "Difference in Magnitudes: 0.0035858154296875\n",
      "\n",
      "Layer: bert.encoder.layer.10.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 19.057435989379883\n",
      "Model 2 Weight Magnitude (Frobenius norm): 19.056360244750977\n",
      "Difference in Magnitudes: 0.00107574462890625\n",
      "\n",
      "Layer: bert.encoder.layer.11.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 54.79314041137695\n",
      "Model 2 Weight Magnitude (Frobenius norm): 54.78507995605469\n",
      "Difference in Magnitudes: 0.008060455322265625\n",
      "\n",
      "Layer: bert.encoder.layer.11.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 54.34776306152344\n",
      "Model 2 Weight Magnitude (Frobenius norm): 54.34064865112305\n",
      "Difference in Magnitudes: 0.007114410400390625\n",
      "\n",
      "Layer: bert.encoder.layer.11.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 35.4284782409668\n",
      "Model 2 Weight Magnitude (Frobenius norm): 35.4251708984375\n",
      "Difference in Magnitudes: 0.003307342529296875\n",
      "\n",
      "Layer: bert.encoder.layer.11.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 35.08650588989258\n",
      "Model 2 Weight Magnitude (Frobenius norm): 35.08333969116211\n",
      "Difference in Magnitudes: 0.00316619873046875\n",
      "\n",
      "Layer: bert.encoder.layer.11.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.95493507385254\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.95458221435547\n",
      "Difference in Magnitudes: 0.0003528594970703125\n",
      "\n",
      "Layer: bert.encoder.layer.11.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 95.89207458496094\n",
      "Model 2 Weight Magnitude (Frobenius norm): 95.88317108154297\n",
      "Difference in Magnitudes: 0.00890350341796875\n",
      "\n",
      "Layer: bert.encoder.layer.11.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 82.9218978881836\n",
      "Model 2 Weight Magnitude (Frobenius norm): 82.91975402832031\n",
      "Difference in Magnitudes: 0.00214385986328125\n",
      "\n",
      "Layer: bert.encoder.layer.11.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.04985237121582\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.050493240356445\n",
      "Difference in Magnitudes: 0.000640869140625\n",
      "\n",
      "Layer: bert.encoder.layer.12.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 57.23074722290039\n",
      "Model 2 Weight Magnitude (Frobenius norm): 57.228614807128906\n",
      "Difference in Magnitudes: 0.002132415771484375\n",
      "\n",
      "Layer: bert.encoder.layer.12.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 56.65971374511719\n",
      "Model 2 Weight Magnitude (Frobenius norm): 56.6580810546875\n",
      "Difference in Magnitudes: 0.0016326904296875\n",
      "\n",
      "Layer: bert.encoder.layer.12.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 37.81793212890625\n",
      "Model 2 Weight Magnitude (Frobenius norm): 37.81531524658203\n",
      "Difference in Magnitudes: 0.00261688232421875\n",
      "\n",
      "Layer: bert.encoder.layer.12.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 39.15660095214844\n",
      "Model 2 Weight Magnitude (Frobenius norm): 39.15375900268555\n",
      "Difference in Magnitudes: 0.002841949462890625\n",
      "\n",
      "Layer: bert.encoder.layer.12.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.870885848999023\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.87075424194336\n",
      "Difference in Magnitudes: 0.0001316070556640625\n",
      "\n",
      "Layer: bert.encoder.layer.12.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 95.09252166748047\n",
      "Model 2 Weight Magnitude (Frobenius norm): 95.08546447753906\n",
      "Difference in Magnitudes: 0.00705718994140625\n",
      "\n",
      "Layer: bert.encoder.layer.12.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 82.48839569091797\n",
      "Model 2 Weight Magnitude (Frobenius norm): 82.48743438720703\n",
      "Difference in Magnitudes: 0.0009613037109375\n",
      "\n",
      "Layer: bert.encoder.layer.12.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 17.626773834228516\n",
      "Model 2 Weight Magnitude (Frobenius norm): 17.625850677490234\n",
      "Difference in Magnitudes: 0.00092315673828125\n",
      "\n",
      "Layer: bert.encoder.layer.13.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 57.84699630737305\n",
      "Model 2 Weight Magnitude (Frobenius norm): 57.84111785888672\n",
      "Difference in Magnitudes: 0.005878448486328125\n",
      "\n",
      "Layer: bert.encoder.layer.13.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 57.58399200439453\n",
      "Model 2 Weight Magnitude (Frobenius norm): 57.578975677490234\n",
      "Difference in Magnitudes: 0.005016326904296875\n",
      "\n",
      "Layer: bert.encoder.layer.13.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 39.141605377197266\n",
      "Model 2 Weight Magnitude (Frobenius norm): 39.13820266723633\n",
      "Difference in Magnitudes: 0.0034027099609375\n",
      "\n",
      "Layer: bert.encoder.layer.13.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 39.2091178894043\n",
      "Model 2 Weight Magnitude (Frobenius norm): 39.205692291259766\n",
      "Difference in Magnitudes: 0.00342559814453125\n",
      "\n",
      "Layer: bert.encoder.layer.13.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.88212013244629\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.88218879699707\n",
      "Difference in Magnitudes: 6.866455078125e-05\n",
      "\n",
      "Layer: bert.encoder.layer.13.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 92.34093475341797\n",
      "Model 2 Weight Magnitude (Frobenius norm): 92.33202362060547\n",
      "Difference in Magnitudes: 0.0089111328125\n",
      "\n",
      "Layer: bert.encoder.layer.13.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 80.06409454345703\n",
      "Model 2 Weight Magnitude (Frobenius norm): 80.06380462646484\n",
      "Difference in Magnitudes: 0.0002899169921875\n",
      "\n",
      "Layer: bert.encoder.layer.13.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 17.984888076782227\n",
      "Model 2 Weight Magnitude (Frobenius norm): 17.98617935180664\n",
      "Difference in Magnitudes: 0.0012912750244140625\n",
      "\n",
      "Layer: bert.encoder.layer.14.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 61.44041061401367\n",
      "Model 2 Weight Magnitude (Frobenius norm): 61.43670654296875\n",
      "Difference in Magnitudes: 0.003704071044921875\n",
      "\n",
      "Layer: bert.encoder.layer.14.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 60.08771896362305\n",
      "Model 2 Weight Magnitude (Frobenius norm): 60.08408737182617\n",
      "Difference in Magnitudes: 0.003631591796875\n",
      "\n",
      "Layer: bert.encoder.layer.14.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 36.86115646362305\n",
      "Model 2 Weight Magnitude (Frobenius norm): 36.859344482421875\n",
      "Difference in Magnitudes: 0.001811981201171875\n",
      "\n",
      "Layer: bert.encoder.layer.14.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 37.55573654174805\n",
      "Model 2 Weight Magnitude (Frobenius norm): 37.55403137207031\n",
      "Difference in Magnitudes: 0.001705169677734375\n",
      "\n",
      "Layer: bert.encoder.layer.14.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.8137264251709\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.81432342529297\n",
      "Difference in Magnitudes: 0.0005970001220703125\n",
      "\n",
      "Layer: bert.encoder.layer.14.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 92.28922271728516\n",
      "Model 2 Weight Magnitude (Frobenius norm): 92.28633117675781\n",
      "Difference in Magnitudes: 0.00289154052734375\n",
      "\n",
      "Layer: bert.encoder.layer.14.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 76.46913146972656\n",
      "Model 2 Weight Magnitude (Frobenius norm): 76.46952056884766\n",
      "Difference in Magnitudes: 0.00038909912109375\n",
      "\n",
      "Layer: bert.encoder.layer.14.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 17.704635620117188\n",
      "Model 2 Weight Magnitude (Frobenius norm): 17.705129623413086\n",
      "Difference in Magnitudes: 0.0004940032958984375\n",
      "\n",
      "Layer: bert.encoder.layer.15.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 58.475032806396484\n",
      "Model 2 Weight Magnitude (Frobenius norm): 58.47114562988281\n",
      "Difference in Magnitudes: 0.003887176513671875\n",
      "\n",
      "Layer: bert.encoder.layer.15.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 58.21622085571289\n",
      "Model 2 Weight Magnitude (Frobenius norm): 58.21403121948242\n",
      "Difference in Magnitudes: 0.00218963623046875\n",
      "\n",
      "Layer: bert.encoder.layer.15.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 38.181129455566406\n",
      "Model 2 Weight Magnitude (Frobenius norm): 38.178104400634766\n",
      "Difference in Magnitudes: 0.003025054931640625\n",
      "\n",
      "Layer: bert.encoder.layer.15.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 37.039180755615234\n",
      "Model 2 Weight Magnitude (Frobenius norm): 37.036170959472656\n",
      "Difference in Magnitudes: 0.003009796142578125\n",
      "\n",
      "Layer: bert.encoder.layer.15.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.455904006958008\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.45633888244629\n",
      "Difference in Magnitudes: 0.00043487548828125\n",
      "\n",
      "Layer: bert.encoder.layer.15.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 88.55142974853516\n",
      "Model 2 Weight Magnitude (Frobenius norm): 88.54881286621094\n",
      "Difference in Magnitudes: 0.00261688232421875\n",
      "\n",
      "Layer: bert.encoder.layer.15.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 74.26004791259766\n",
      "Model 2 Weight Magnitude (Frobenius norm): 74.2585678100586\n",
      "Difference in Magnitudes: 0.0014801025390625\n",
      "\n",
      "Layer: bert.encoder.layer.15.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 17.445274353027344\n",
      "Model 2 Weight Magnitude (Frobenius norm): 17.444902420043945\n",
      "Difference in Magnitudes: 0.0003719329833984375\n",
      "\n",
      "Layer: bert.encoder.layer.16.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 59.276248931884766\n",
      "Model 2 Weight Magnitude (Frobenius norm): 59.274810791015625\n",
      "Difference in Magnitudes: 0.001438140869140625\n",
      "\n",
      "Layer: bert.encoder.layer.16.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 59.02914810180664\n",
      "Model 2 Weight Magnitude (Frobenius norm): 59.02785873413086\n",
      "Difference in Magnitudes: 0.00128936767578125\n",
      "\n",
      "Layer: bert.encoder.layer.16.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 38.69740295410156\n",
      "Model 2 Weight Magnitude (Frobenius norm): 38.694969177246094\n",
      "Difference in Magnitudes: 0.00243377685546875\n",
      "\n",
      "Layer: bert.encoder.layer.16.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 36.74565124511719\n",
      "Model 2 Weight Magnitude (Frobenius norm): 36.74372100830078\n",
      "Difference in Magnitudes: 0.00193023681640625\n",
      "\n",
      "Layer: bert.encoder.layer.16.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 19.19600486755371\n",
      "Model 2 Weight Magnitude (Frobenius norm): 19.195566177368164\n",
      "Difference in Magnitudes: 0.000438690185546875\n",
      "\n",
      "Layer: bert.encoder.layer.16.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 88.99528503417969\n",
      "Model 2 Weight Magnitude (Frobenius norm): 88.99070739746094\n",
      "Difference in Magnitudes: 0.00457763671875\n",
      "\n",
      "Layer: bert.encoder.layer.16.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 73.11268615722656\n",
      "Model 2 Weight Magnitude (Frobenius norm): 73.11051177978516\n",
      "Difference in Magnitudes: 0.00217437744140625\n",
      "\n",
      "Layer: bert.encoder.layer.16.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.890727996826172\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.889556884765625\n",
      "Difference in Magnitudes: 0.001171112060546875\n",
      "\n",
      "Layer: bert.encoder.layer.17.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 60.188804626464844\n",
      "Model 2 Weight Magnitude (Frobenius norm): 60.184444427490234\n",
      "Difference in Magnitudes: 0.004360198974609375\n",
      "\n",
      "Layer: bert.encoder.layer.17.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 59.28314208984375\n",
      "Model 2 Weight Magnitude (Frobenius norm): 59.28066635131836\n",
      "Difference in Magnitudes: 0.002475738525390625\n",
      "\n",
      "Layer: bert.encoder.layer.17.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 37.28621292114258\n",
      "Model 2 Weight Magnitude (Frobenius norm): 37.28337478637695\n",
      "Difference in Magnitudes: 0.002838134765625\n",
      "\n",
      "Layer: bert.encoder.layer.17.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 36.17548370361328\n",
      "Model 2 Weight Magnitude (Frobenius norm): 36.172607421875\n",
      "Difference in Magnitudes: 0.00287628173828125\n",
      "\n",
      "Layer: bert.encoder.layer.17.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 19.639554977416992\n",
      "Model 2 Weight Magnitude (Frobenius norm): 19.638137817382812\n",
      "Difference in Magnitudes: 0.0014171600341796875\n",
      "\n",
      "Layer: bert.encoder.layer.17.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 87.68512725830078\n",
      "Model 2 Weight Magnitude (Frobenius norm): 87.6796875\n",
      "Difference in Magnitudes: 0.00543975830078125\n",
      "\n",
      "Layer: bert.encoder.layer.17.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 72.13983154296875\n",
      "Model 2 Weight Magnitude (Frobenius norm): 72.13768005371094\n",
      "Difference in Magnitudes: 0.0021514892578125\n",
      "\n",
      "Layer: bert.encoder.layer.17.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 17.97601890563965\n",
      "Model 2 Weight Magnitude (Frobenius norm): 17.97427749633789\n",
      "Difference in Magnitudes: 0.0017414093017578125\n",
      "\n",
      "Layer: bert.encoder.layer.18.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 59.09600067138672\n",
      "Model 2 Weight Magnitude (Frobenius norm): 59.09087371826172\n",
      "Difference in Magnitudes: 0.005126953125\n",
      "\n",
      "Layer: bert.encoder.layer.18.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 58.82730484008789\n",
      "Model 2 Weight Magnitude (Frobenius norm): 58.82315444946289\n",
      "Difference in Magnitudes: 0.004150390625\n",
      "\n",
      "Layer: bert.encoder.layer.18.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 40.55985641479492\n",
      "Model 2 Weight Magnitude (Frobenius norm): 40.55585479736328\n",
      "Difference in Magnitudes: 0.004001617431640625\n",
      "\n",
      "Layer: bert.encoder.layer.18.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 37.768760681152344\n",
      "Model 2 Weight Magnitude (Frobenius norm): 37.764469146728516\n",
      "Difference in Magnitudes: 0.004291534423828125\n",
      "\n",
      "Layer: bert.encoder.layer.18.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 20.41456413269043\n",
      "Model 2 Weight Magnitude (Frobenius norm): 20.41284942626953\n",
      "Difference in Magnitudes: 0.0017147064208984375\n",
      "\n",
      "Layer: bert.encoder.layer.18.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 87.20532989501953\n",
      "Model 2 Weight Magnitude (Frobenius norm): 87.20014953613281\n",
      "Difference in Magnitudes: 0.00518035888671875\n",
      "\n",
      "Layer: bert.encoder.layer.18.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 70.697998046875\n",
      "Model 2 Weight Magnitude (Frobenius norm): 70.69571685791016\n",
      "Difference in Magnitudes: 0.00228118896484375\n",
      "\n",
      "Layer: bert.encoder.layer.18.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.08027458190918\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.078718185424805\n",
      "Difference in Magnitudes: 0.001556396484375\n",
      "\n",
      "Layer: bert.encoder.layer.19.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 59.993614196777344\n",
      "Model 2 Weight Magnitude (Frobenius norm): 59.99040222167969\n",
      "Difference in Magnitudes: 0.00321197509765625\n",
      "\n",
      "Layer: bert.encoder.layer.19.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 59.806495666503906\n",
      "Model 2 Weight Magnitude (Frobenius norm): 59.804996490478516\n",
      "Difference in Magnitudes: 0.001499176025390625\n",
      "\n",
      "Layer: bert.encoder.layer.19.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 39.696834564208984\n",
      "Model 2 Weight Magnitude (Frobenius norm): 39.69290542602539\n",
      "Difference in Magnitudes: 0.00392913818359375\n",
      "\n",
      "Layer: bert.encoder.layer.19.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 36.571495056152344\n",
      "Model 2 Weight Magnitude (Frobenius norm): 36.56801986694336\n",
      "Difference in Magnitudes: 0.003475189208984375\n",
      "\n",
      "Layer: bert.encoder.layer.19.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 20.70287322998047\n",
      "Model 2 Weight Magnitude (Frobenius norm): 20.7000732421875\n",
      "Difference in Magnitudes: 0.00279998779296875\n",
      "\n",
      "Layer: bert.encoder.layer.19.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 88.55443572998047\n",
      "Model 2 Weight Magnitude (Frobenius norm): 88.54354095458984\n",
      "Difference in Magnitudes: 0.010894775390625\n",
      "\n",
      "Layer: bert.encoder.layer.19.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 71.39740753173828\n",
      "Model 2 Weight Magnitude (Frobenius norm): 71.39456939697266\n",
      "Difference in Magnitudes: 0.002838134765625\n",
      "\n",
      "Layer: bert.encoder.layer.19.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.650339126586914\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.647598266601562\n",
      "Difference in Magnitudes: 0.0027408599853515625\n",
      "\n",
      "Layer: bert.encoder.layer.20.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 58.51848602294922\n",
      "Model 2 Weight Magnitude (Frobenius norm): 58.51527786254883\n",
      "Difference in Magnitudes: 0.003208160400390625\n",
      "\n",
      "Layer: bert.encoder.layer.20.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 57.989036560058594\n",
      "Model 2 Weight Magnitude (Frobenius norm): 57.98674392700195\n",
      "Difference in Magnitudes: 0.002292633056640625\n",
      "\n",
      "Layer: bert.encoder.layer.20.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 37.89768981933594\n",
      "Model 2 Weight Magnitude (Frobenius norm): 37.89297103881836\n",
      "Difference in Magnitudes: 0.004718780517578125\n",
      "\n",
      "Layer: bert.encoder.layer.20.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 35.5545768737793\n",
      "Model 2 Weight Magnitude (Frobenius norm): 35.5501594543457\n",
      "Difference in Magnitudes: 0.00441741943359375\n",
      "\n",
      "Layer: bert.encoder.layer.20.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 21.14794921875\n",
      "Model 2 Weight Magnitude (Frobenius norm): 21.144304275512695\n",
      "Difference in Magnitudes: 0.0036449432373046875\n",
      "\n",
      "Layer: bert.encoder.layer.20.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 89.8519515991211\n",
      "Model 2 Weight Magnitude (Frobenius norm): 89.83672332763672\n",
      "Difference in Magnitudes: 0.015228271484375\n",
      "\n",
      "Layer: bert.encoder.layer.20.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 71.0179214477539\n",
      "Model 2 Weight Magnitude (Frobenius norm): 71.0151138305664\n",
      "Difference in Magnitudes: 0.0028076171875\n",
      "\n",
      "Layer: bert.encoder.layer.20.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.941818237304688\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.94041633605957\n",
      "Difference in Magnitudes: 0.0014019012451171875\n",
      "\n",
      "Layer: bert.encoder.layer.21.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 56.52125549316406\n",
      "Model 2 Weight Magnitude (Frobenius norm): 56.51759719848633\n",
      "Difference in Magnitudes: 0.003658294677734375\n",
      "\n",
      "Layer: bert.encoder.layer.21.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 56.26944351196289\n",
      "Model 2 Weight Magnitude (Frobenius norm): 56.26559829711914\n",
      "Difference in Magnitudes: 0.00384521484375\n",
      "\n",
      "Layer: bert.encoder.layer.21.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 37.1525993347168\n",
      "Model 2 Weight Magnitude (Frobenius norm): 37.1474609375\n",
      "Difference in Magnitudes: 0.005138397216796875\n",
      "\n",
      "Layer: bert.encoder.layer.21.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 36.149417877197266\n",
      "Model 2 Weight Magnitude (Frobenius norm): 36.14443588256836\n",
      "Difference in Magnitudes: 0.00498199462890625\n",
      "\n",
      "Layer: bert.encoder.layer.21.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 22.132720947265625\n",
      "Model 2 Weight Magnitude (Frobenius norm): 22.1298828125\n",
      "Difference in Magnitudes: 0.002838134765625\n",
      "\n",
      "Layer: bert.encoder.layer.21.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 84.37647247314453\n",
      "Model 2 Weight Magnitude (Frobenius norm): 84.3635025024414\n",
      "Difference in Magnitudes: 0.012969970703125\n",
      "\n",
      "Layer: bert.encoder.layer.21.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 69.63375854492188\n",
      "Model 2 Weight Magnitude (Frobenius norm): 69.63249969482422\n",
      "Difference in Magnitudes: 0.00125885009765625\n",
      "\n",
      "Layer: bert.encoder.layer.21.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.385147094726562\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.38239860534668\n",
      "Difference in Magnitudes: 0.0027484893798828125\n",
      "\n",
      "Layer: bert.encoder.layer.22.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 56.12117004394531\n",
      "Model 2 Weight Magnitude (Frobenius norm): 56.1099967956543\n",
      "Difference in Magnitudes: 0.011173248291015625\n",
      "\n",
      "Layer: bert.encoder.layer.22.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 55.47282028198242\n",
      "Model 2 Weight Magnitude (Frobenius norm): 55.46194839477539\n",
      "Difference in Magnitudes: 0.01087188720703125\n",
      "\n",
      "Layer: bert.encoder.layer.22.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 39.07271957397461\n",
      "Model 2 Weight Magnitude (Frobenius norm): 39.06529235839844\n",
      "Difference in Magnitudes: 0.007427215576171875\n",
      "\n",
      "Layer: bert.encoder.layer.22.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 38.56005096435547\n",
      "Model 2 Weight Magnitude (Frobenius norm): 38.552696228027344\n",
      "Difference in Magnitudes: 0.007354736328125\n",
      "\n",
      "Layer: bert.encoder.layer.22.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 22.315343856811523\n",
      "Model 2 Weight Magnitude (Frobenius norm): 22.312969207763672\n",
      "Difference in Magnitudes: 0.0023746490478515625\n",
      "\n",
      "Layer: bert.encoder.layer.22.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 77.8931655883789\n",
      "Model 2 Weight Magnitude (Frobenius norm): 77.88324737548828\n",
      "Difference in Magnitudes: 0.009918212890625\n",
      "\n",
      "Layer: bert.encoder.layer.22.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 66.38822174072266\n",
      "Model 2 Weight Magnitude (Frobenius norm): 66.38731384277344\n",
      "Difference in Magnitudes: 0.00090789794921875\n",
      "\n",
      "Layer: bert.encoder.layer.22.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.843955993652344\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.847257614135742\n",
      "Difference in Magnitudes: 0.0033016204833984375\n",
      "\n",
      "Layer: bert.encoder.layer.23.attention.self.query\n",
      "Model 1 Weight Magnitude (Frobenius norm): 56.0069694519043\n",
      "Model 2 Weight Magnitude (Frobenius norm): 56.015045166015625\n",
      "Difference in Magnitudes: 0.008075714111328125\n",
      "\n",
      "Layer: bert.encoder.layer.23.attention.self.key\n",
      "Model 1 Weight Magnitude (Frobenius norm): 54.80339813232422\n",
      "Model 2 Weight Magnitude (Frobenius norm): 54.81086349487305\n",
      "Difference in Magnitudes: 0.007465362548828125\n",
      "\n",
      "Layer: bert.encoder.layer.23.attention.self.value\n",
      "Model 1 Weight Magnitude (Frobenius norm): 40.49129867553711\n",
      "Model 2 Weight Magnitude (Frobenius norm): 40.486534118652344\n",
      "Difference in Magnitudes: 0.004764556884765625\n",
      "\n",
      "Layer: bert.encoder.layer.23.attention.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 38.74618911743164\n",
      "Model 2 Weight Magnitude (Frobenius norm): 38.74006271362305\n",
      "Difference in Magnitudes: 0.00612640380859375\n",
      "\n",
      "Layer: bert.encoder.layer.23.attention.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 20.594419479370117\n",
      "Model 2 Weight Magnitude (Frobenius norm): 20.589706420898438\n",
      "Difference in Magnitudes: 0.0047130584716796875\n",
      "\n",
      "Layer: bert.encoder.layer.23.intermediate.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 71.7383041381836\n",
      "Model 2 Weight Magnitude (Frobenius norm): 71.73944091796875\n",
      "Difference in Magnitudes: 0.00113677978515625\n",
      "\n",
      "Layer: bert.encoder.layer.23.output.dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 51.18300247192383\n",
      "Model 2 Weight Magnitude (Frobenius norm): 51.19477844238281\n",
      "Difference in Magnitudes: 0.011775970458984375\n",
      "\n",
      "Layer: bert.encoder.layer.23.output.LayerNorm\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.132644653320312\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.283647537231445\n",
      "Difference in Magnitudes: 0.1510028839111328\n",
      "\n",
      "Layer: dense\n",
      "Model 1 Weight Magnitude (Frobenius norm): 18.47492790222168\n",
      "Model 2 Weight Magnitude (Frobenius norm): 18.698888778686523\n",
      "Difference in Magnitudes: 0.22396087646484375\n",
      "\n",
      "Layer: classifier\n",
      "Model 1 Weight Magnitude (Frobenius norm): 2.0878589153289795\n",
      "Model 2 Weight Magnitude (Frobenius norm): 2.2199130058288574\n",
      "Difference in Magnitudes: 0.13205409049987793\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_weight_magnitudes(model, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0370,  0.0153,  0.0188,  ..., -0.0121,  0.0532,  0.0256],\n",
       "        [-0.0752, -0.0420, -0.0308,  ..., -0.0207,  0.0266,  0.0622],\n",
       "        [ 0.0109,  0.0734, -0.0455,  ...,  0.0307,  0.0141,  0.0621],\n",
       "        ...,\n",
       "        [-0.0215, -0.0407, -0.0288,  ...,  0.0053,  0.0627,  0.0069],\n",
       "        [ 0.0576, -0.0488,  0.0196,  ..., -0.0574, -0.0502, -0.0313],\n",
       "        [-0.0638,  0.0228, -0.0646,  ...,  0.0100,  0.0714, -0.0597]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.nn.init.xavier_uniform_(model.classifier.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0070, -0.0028,  0.0200,  ...,  0.0216, -0.0230,  0.0187],\n",
       "        [ 0.0020, -0.0092, -0.0128,  ..., -0.0070, -0.0008, -0.0073],\n",
       "        [ 0.0090, -0.0141, -0.0174,  ...,  0.0209, -0.0195, -0.0021],\n",
       "        ...,\n",
       "        [ 0.0120, -0.0242,  0.0238,  ..., -0.0100, -0.0196,  0.0247],\n",
       "        [ 0.0019, -0.0030, -0.0235,  ..., -0.0257,  0.0282, -0.0026],\n",
       "        [-0.0295,  0.0046, -0.0225,  ..., -0.0286, -0.0205, -0.0068]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0274, -0.0410, -0.0128, -0.0804, -0.0125,  0.0043,  0.0294, -0.0007,\n",
       "         0.0005,  0.0273,  0.0639,  0.0117,  0.0823], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.crf.end_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-linking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
